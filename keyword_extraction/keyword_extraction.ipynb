{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_word_segment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XxjdNYAI9fuBcxUwx-u1yyXtBwqn3HV2",
      "authorship_tag": "ABX9TyPQsJVTH+0/qG9GeEE7z/BR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/bert_project/blob/main/keyword_extraction/keyword_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnm6di42lNnq"
      },
      "source": [
        "# 原始功能:BERT做无监督分词(由 蘇劍林 提供)\r\n",
        "# 介绍：https://kexue.fm/archives/7476\r\n",
        "# 我將分詞功能擴充為:檢查兩個中文詞組間的相關性，以判斷是否為連接的新詞(例如:機器 學習)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNL4IAlKyF89"
      },
      "source": [
        "#輸入段落\r\n",
        "text = \"機器學習基本上可以稱為聖經之類的書\""
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKWFFmeT6iYJ",
        "outputId": "45d9978e-28a6-4ebb-e1bc-5e8cca6b616f"
      },
      "source": [
        "# 安裝keras-bert函式庫以及BERT-wwm模型\n",
        "!pip -q install keras-bert\n",
        "!pip -q install keras-transformer\n",
        "!gdown --id \"1SQJZmilEwCqffyb4MbQBPlr3-N20sLnS\" --output chinese_wwm_L-12_H-768_A-12.zip\n",
        "!unzip  chinese_wwm_L-12_H-768_A-12.zip"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SQJZmilEwCqffyb4MbQBPlr3-N20sLnS\n",
            "To: /content/chinese_wwm_L-12_H-768_A-12.zip\n",
            "382MB [00:02, 173MB/s]\n",
            "Archive:  chinese_wwm_L-12_H-768_A-12.zip\n",
            "replace publish/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saed3h4C5HJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b10c23-f269-485c-8043-f4a8f781d719"
      },
      "source": [
        "# 設置BERT的編碼器\n",
        "config_path = 'publish/bert_config.json'\n",
        "checkpoint_path = 'publish/bert_model.ckpt'\n",
        "vocab_path = 'publish/vocab.txt'\n",
        "token_dict = {}\n",
        "with open(vocab_path, 'r', encoding='utf8') as f:\n",
        "    for line in f.readlines():\n",
        "        token = line.strip()\n",
        "        token_dict[token] = len(token_dict)\n",
        "print(\"辭典長度:\", len(token_dict))\n",
        "from keras_bert import Tokenizer\n",
        "tokenizer = Tokenizer(token_dict)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "辭典長度: 21128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JQifTce4gUG",
        "outputId": "04055923-e19a-44f4-ed4c-8187e0bff7ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#預處理:將段落文字轉為簡體,濾除英文,並以標點符號切分句子(句子之間被標點隔開,前後的詞不可能是相連的新詞)\r\n",
        "!pip install opencc-python-reimplemented\r\n",
        "from opencc import OpenCC\r\n",
        "import re\r\n",
        "cc = OpenCC('t2s')\r\n",
        "#標點符號集合\r\n",
        "punct = \"，,|:|[|]|。|、|…\"\r\n",
        "r1 = '[a-zA-Z0-9]' \r\n",
        "text = re.sub(r1, '', text)\r\n",
        "text_convert = cc.convert(text)\r\n",
        "sentence_list = re.split(punct,text_convert)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencc-python-reimplemented in /usr/local/lib/python3.6/dist-packages (0.1.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU6WQK2Z6823"
      },
      "source": [
        "#載入jieba字典\r\n",
        "import jieba.analyse\r\n",
        "import os\r\n",
        "from urllib.request import urlretrieve\r\n",
        "if not os.path.exists (\"dict.txt.big\"):\r\n",
        "  url = \"https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big\"\r\n",
        "  urlretrieve(url, \"dict.txt.big\")\r\n",
        "jieba.set_dictionary ('dict.txt.big')"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlf7p9gud_EZ",
        "outputId": "e99a85ed-285d-423d-8a55-9e13ee4638e3"
      },
      "source": [
        "#使用jieba分詞,輸出word_pair_list,,由欲處理的詞組組成\n",
        "word_pair_list=[]\n",
        "for n in sentence_list:\n",
        "  words = \"\".join(filter(lambda x: x not in punct,jieba.cut(n)))\n",
        "  sentence_cut =  jieba.lcut(words)\n",
        "  for m in range(len(sentence_cut)-1):\n",
        "    sentence_word = [sentence_cut[m],sentence_cut[m+1]]\n",
        "    word_pair_list.append(sentence_word)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from /content/dict.txt.big ...\n",
            "Loading model from cache /tmp/jieba.u501edca284da514cb68b53a20324f4e3.cache\n",
            "Loading model cost 1.789 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd6_dh99v0YC"
      },
      "source": [
        "#輸出paragraph,待處理的整個段落\r\n",
        "text_convert_list = jieba.lcut(text_convert)\r\n",
        "words = []\r\n",
        "for word in text_convert_list:\r\n",
        "      if word not in punct:          \r\n",
        "          words.append(word)\r\n",
        "paragraph = \"\".join(words)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvWMxq_TcMPq"
      },
      "source": [
        "#準備標籤step1,此函數可以標記目標詞在段落中的位置(需要依序輸入word_pair_list中的元素,再和paragraph比較)\n",
        "def word_position(content,keyword):\n",
        "  keyword_set = set(keyword)\n",
        "  content_word_segment= []\n",
        "  content = jieba.lcut(content)\n",
        "  words = []\n",
        "  for word in content:\n",
        "        if word not in punct:          \n",
        "            words.append(word)\n",
        "  content = words\n",
        "  for n in content:\n",
        "    test, se = tokenizer.encode(n)\n",
        "    len_of_token = (len(test)-2)\n",
        "    if len_of_token>0:\n",
        "      if n in keyword_set:\n",
        "        content_word_segment.append(100+len_of_token)\n",
        "      else:\n",
        "        try:\n",
        "          x = n.lower()\n",
        "          if x in keyword_set:\n",
        "            content_word_segment.append(100+len_of_token)\n",
        "          else:\n",
        "            content_word_segment.append(len_of_token)\n",
        "        except:\n",
        "          content_word_segment.append(len_of_token)\n",
        "  return np.array(content_word_segment)"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP0ZVzcpcd1w"
      },
      "source": [
        "#設定BERT輸入的句子長度\r\n",
        "SEQ_LEN = len(paragraph)+2"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl93L1ZVcSDT"
      },
      "source": [
        "#準備標籤step2,根據step1的結果,輸出元素為0和1的list,1代表該位置的字為目標詞的一部分,list長度=paragraph長度\n",
        "def prepare_label(content_word_segment:np.array):\n",
        "  content_char_segment= np.array([0])\n",
        "  for n in content_word_segment:\n",
        "    if n >100:\n",
        "      content_char_segment = np.concatenate([content_char_segment,np.ones(n-100)])\n",
        "    else:\n",
        "      content_char_segment = np.concatenate([content_char_segment,np.zeros(n)])\n",
        "  content_char_segment = np.concatenate([content_char_segment,np.zeros(SEQ_LEN)])\n",
        "  content_char_segment = content_char_segment[:SEQ_LEN]\n",
        "  return content_char_segment"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-nu9DyNyFVX"
      },
      "source": [
        "#開始標記,word_pair_list中每一個詞組會產生3個list,分別用0和1表示詞1,整個詞組,詞2在paragraph中的位置,輸出標記position_list\n",
        "position_list = []\n",
        "for n in word_pair_list:\n",
        "  position_list.append(prepare_label(word_position(paragraph,[n[0]])))\n",
        "  position_list.append(prepare_label(word_position(paragraph,n)))\n",
        "  position_list.append(prepare_label(word_position(paragraph,[n[1]])))"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oa-3ftyyoy3"
      },
      "source": [
        "#此函數輸入段落編碼(下一欄的ids)並mask在position_list中值為1的對應位置\n",
        "def mask(ids:list,mask_terms:list):\n",
        "  for i in range(len(mask_terms)):\n",
        "    if mask_terms[i]==1:\n",
        "      ids[i]=103\n",
        "  return ids"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ONt8cU4UYqc"
      },
      "source": [
        "#準備BERT模型的二項輸入,第一項為句子的編碼ids,將ids代入mask函數,mask相對應的詞組\n",
        "input = []\n",
        "for n in range(len(position_list)):\n",
        "  ids,seg = tokenizer.encode(paragraph)\n",
        "  input.append(mask(ids,position_list[n]))\n",
        "import tensorflow as tf\n",
        "input_ids = tf.convert_to_tensor(input, dtype=tf.int32)\n",
        "input_seg = tf.convert_to_tensor(np.zeros_like(input), dtype=tf.int32)"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCM83oAoxufH"
      },
      "source": [
        "# 配置BERT模型\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "model = load_trained_model_from_checkpoint(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    training=False,\n",
        "    trainable=False,\n",
        "    seq_len=SEQ_LEN,\n",
        "    )"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUv_cyA3In-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822236fd-b04a-49d7-d8c6-3bf39a802596"
      },
      "source": [
        "#使用BERT訓練過的詞向量預測被mask的詞組\r\n",
        "vectors = model.predict([input_ids, input_seg])"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 22 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5bd28f1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwzEE_hT3oX"
      },
      "source": [
        "#準備關聯性列表\r\n",
        "d_list = []"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8UENTUWa9RB"
      },
      "source": [
        "#定義歐式距離函數\n",
        "import numpy as np\n",
        "def dist(x, y):\n",
        "    return np.sqrt(((x - y)**2).sum())"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uop_7NHsIXIj"
      },
      "source": [
        "#計算每一詞組中,二個詞的關聯性,將結果輸出到關聯性列表d_list中\n",
        "import numpy as np\n",
        "threshold = 10\n",
        "for n in range(1, round(len(position_list)/3+1)):\n",
        "    v1 = np.zeros_like(vectors.shape[2]).astype('float32')\n",
        "    v2_1 = np.zeros_like(vectors.shape[2]).astype('float32')\n",
        "    v2_2 = np.zeros_like(vectors.shape[2]).astype('float32')\n",
        "    v3 = np.zeros_like(vectors.shape[2]).astype('float32')\n",
        "    len_of_token_1 = 0\n",
        "    len_of_token_2 = 0\n",
        "    for m in range(len(position_list[3*n-2])):\n",
        "      if position_list[3*n-3][m]==1:\n",
        "        v1 = v1 + vectors[3 * n-3][m]\n",
        "        v2_1 = v2_1 + vectors[3 * n-2][m]\n",
        "        len_of_token_1 += 1\n",
        "      elif position_list[3*n-1][m]==1:\n",
        "        v3 = v3 + vectors[3 * n-1][m]\n",
        "        v2_2 = v2_2 + vectors[3 * n-2][m]\n",
        "        len_of_token_2 += 1\n",
        "    v1 = v1/len_of_token_1\n",
        "    v2_1 = v2_1/len_of_token_1\n",
        "    v2_2 = v2_2/len_of_token_2\n",
        "    v3 = v3/len_of_token_2  \n",
        "    d1 = dist(v1, v2_1)\n",
        "    d2 = dist(v2_2 ,v3)\n",
        "    d = (d1 + d2) / 2\n",
        "    d_list.append(d)"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4KeAWg1TZUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdbe947e-114d-4247-a9d2-8debe08a9046"
      },
      "source": [
        "#讀取濾除字表\r\n",
        "!gdown --id \"1VloMIYrUdlKbPutAkmFMgQT-1vc3N9-J\"\r\n",
        "stop_words_list = []\r\n",
        "with open('stop_words.txt', 'r') as f:\r\n",
        "  stop_words_list = [line.strip() for line in f]\r\n",
        "  t = f.readlines()\r\n",
        "  for i in t:\r\n",
        "    stop_words_list.append(cc.convert(i))"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VloMIYrUdlKbPutAkmFMgQT-1vc3N9-J\n",
            "To: /content/stop_words.txt\n",
            "\r  0% 0.00/2.00k [00:00<?, ?B/s]\r100% 2.00k/2.00k [00:00<00:00, 3.35MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUo7L0EcTSpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26af74a9-45db-4ff9-feb7-e8cbab2a648b"
      },
      "source": [
        "#過濾字表中的字後,根據d_list中關連性的大小,尋找對應的詞組,輸出段落中最可能是相連接的詞組\r\n",
        "for n in range(len(d_list)):\r\n",
        "  k = d_list.index(max(d_list))\r\n",
        "  if word_pair_list[k][0] not in stop_words_list and word_pair_list[k][1] not in stop_words_list:\r\n",
        "    with open(\"new_keyword.txt\", \"a\") as output:\r\n",
        "      output.write(\"%s\" % word_pair_list[k])\r\n",
        "    print(word_pair_list[k])\r\n",
        "    break\r\n",
        "  else:\r\n",
        "    del d_list[k]\r\n",
        "    del word_pair_list[k]"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['圣经', '之类']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}