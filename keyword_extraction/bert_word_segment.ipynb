{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_word_segment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XxjdNYAI9fuBcxUwx-u1yyXtBwqn3HV2",
      "authorship_tag": "ABX9TyOCJqPEy7dENU7lhaKT8dxj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shopping-Yuan/bert_project/blob/main/keyword_extraction/bert_word_segment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnm6di42lNnq"
      },
      "source": [
        "# 原始功能:BERT做无监督分词(由蘇劍林提供)\r\n",
        "# 介绍：https://kexue.fm/archives/7476\r\n",
        "# 我將分詞功能擴充為:檢查兩個中文詞組間的相關性，以判斷是否為連接的新詞(例如:機器 學習)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNL4IAlKyF89"
      },
      "source": [
        "#輸入段落\r\n",
        "text = \"BERT的一般用法就是加载其预训练权重，再接一小部分新层，然后在下游任务上进行finetune，换句话说一般的用法都是有监督训练的。基于这个流程，我们可以做中文的分词、NER甚至句法分析，这些想必大家就算没做过也会有所听闻。但如果说直接从预训练的BERT（不finetune）就可以对句子进行分词，甚至析出其句法结构出来，那应该会让人感觉到意外和有趣了。\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKWFFmeT6iYJ",
        "outputId": "a13f20df-f23c-4b8b-a41b-0f5bae6e120a"
      },
      "source": [
        "# 安裝keras-bert函式庫\n",
        "!pip -q install keras-bert\n",
        "!pip -q install keras-transformer\n",
        "!unzip  drive/MyDrive/chinese_wwm_L-12_H-768_A-12.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/MyDrive/chinese_wwm_L-12_H-768_A-12.zip\n",
            "replace publish/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saed3h4C5HJ2"
      },
      "source": [
        "# 設置BERT的編碼器\n",
        "config_path = 'publish/bert_config.json'\n",
        "checkpoint_path = 'publish/bert_model.ckpt'\n",
        "vocab_path = 'publish/vocab.txt'\n",
        "token_dict = {}\n",
        "with open(vocab_path, 'r', encoding='utf8') as f:\n",
        "    for line in f.readlines():\n",
        "        token = line.strip()\n",
        "        token_dict[token] = len(token_dict)\n",
        "print(\"辭典長度:\", len(token_dict))\n",
        "from keras_bert import Tokenizer\n",
        "tokenizer = Tokenizer(token_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlf7p9gud_EZ",
        "outputId": "cd898f2e-dde3-48b9-aab1-9537883abce0"
      },
      "source": [
        "#預處理,皆轉為簡體,使用jieba分詞\n",
        "!pip install opencc-python-reimplemented\n",
        "from opencc import OpenCC\n",
        "import re\n",
        "import jieba.analyse\n",
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "if not os.path.exists (\"dict.txt.big\"):\n",
        "  url = \"https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big\"\n",
        "  urlretrieve(url, \"dict.txt.big\")\n",
        "jieba.set_dictionary ('dict.txt.big')\n",
        "cc = OpenCC('t2s')\n",
        "import re\n",
        "punct = \",|:|[|]|。|、|…\"\n",
        "r1 = '[a-zA-Z0-9]' \n",
        "text = re.sub(r1, '', text)\n",
        "text_convert = cc.convert(text)\n",
        "sentence_list = re.split(punct,text_convert)\n",
        "\n",
        "word_pair_list=[]\n",
        "for n in sentence_list:\n",
        "  words = \"\".join(filter(lambda x: x not in punct,jieba.cut(n)))\n",
        "  sentence_cut =  jieba.lcut(words)\n",
        "  for m in range(len(sentence_cut)-1):\n",
        "    sentence_word = [sentence_cut[m],sentence_cut[m+1]]\n",
        "    word_pair_list.append(sentence_word)\n",
        "\n",
        "text_convert = jieba.lcut(text_convert)\n",
        "words = []\n",
        "for word in text_convert:\n",
        "      if word not in punct:          \n",
        "          words.append(word)\n",
        "text_convert = \"\".join(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencc-python-reimplemented in /usr/local/lib/python3.6/dist-packages (0.1.6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from /content/dict.txt.big ...\n",
            "Loading model from cache /tmp/jieba.u501edca284da514cb68b53a20324f4e3.cache\n",
            "Loading model cost 1.176 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['機器', '學習', '或', '深度', '學習']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP0ZVzcpcd1w"
      },
      "source": [
        "#設定BERT輸入的句子長度\r\n",
        "SEQ_LEN = len(text_convert)+2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvWMxq_TcMPq"
      },
      "source": [
        "#準備標籤step1,先標記關鍵詞的位置\n",
        "def word_position(content,keyword):\n",
        "  keyword_set = set(keyword)\n",
        "  content_word_segment= []\n",
        "  content = jieba.lcut(content)\n",
        "  words = []\n",
        "  for word in content:\n",
        "        if word not in punct:          \n",
        "            words.append(word)\n",
        "  content = words\n",
        "  for n in content:\n",
        "    test, se = tokenizer.encode(n)\n",
        "    len_of_token = (len(test)-2)\n",
        "    if len_of_token>0:\n",
        "      if n in keyword_set:\n",
        "        content_word_segment.append(100+len_of_token)\n",
        "      else:\n",
        "        try:\n",
        "          x = n.lower()\n",
        "          if x in keyword_set:\n",
        "            content_word_segment.append(100+len_of_token)\n",
        "          else:\n",
        "            content_word_segment.append(len_of_token)\n",
        "        except:\n",
        "          content_word_segment.append(len_of_token)\n",
        "  return np.array(content_word_segment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl93L1ZVcSDT"
      },
      "source": [
        "#準備標籤step2,輸出元素為0和1的list,1代表該位置的字為關鍵詞的一部分\n",
        "import numpy as np\n",
        "def prepare_label(content_word_segment:np.array, sq_le = SEQ_LEN):\n",
        "  content_char_segment= np.array([0])\n",
        "  for n in content_word_segment:\n",
        "    if n >100:\n",
        "      content_char_segment = np.concatenate([content_char_segment,np.ones(n-100)])\n",
        "    else:\n",
        "      content_char_segment = np.concatenate([content_char_segment,np.zeros(n)])\n",
        "  # content_char_segment = np.concatenate([content_char_segment,np.zeros(1)])\n",
        "  content_char_segment = np.concatenate([content_char_segment,np.zeros(SEQ_LEN)])\n",
        "  content_char_segment = content_char_segment[:SEQ_LEN]\n",
        "  return content_char_segment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-nu9DyNyFVX"
      },
      "source": [
        "#對每一個詞組,標記位置\n",
        "position_list = []\n",
        "for n in word_pair_list:\n",
        "  position_list.append(prepare_label(word_position(text_convert,[n[0]])))\n",
        "  position_list.append(prepare_label(word_position(text_convert,n)))\n",
        "  position_list.append(prepare_label(word_position(text_convert,[n[1]])))\n",
        "# position_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oa-3ftyyoy3"
      },
      "source": [
        "#本函數可以根據標記mask相對應的位置\n",
        "def mask(ids:list,mask_terms:list):\n",
        "  for i in range(len(mask_terms)):\n",
        "    if mask_terms[i]==1:\n",
        "      ids[i]=103\n",
        "  return ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ONt8cU4UYqc"
      },
      "source": [
        "#準備BERT模型的二項輸入,mask相對應的詞組\n",
        "input = []\n",
        "for n in range(len(position_list)):\n",
        "  ids,seg = tokenizer.encode(text_convert)\n",
        "  input.append(mask(ids,position_list[n]))\n",
        "import tensorflow as tf\n",
        "input_ids = tf.convert_to_tensor(input, dtype=tf.int32)\n",
        "input_seg = tf.convert_to_tensor(np.zeros_like(input), dtype=tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCM83oAoxufH"
      },
      "source": [
        "# 配置BERT模型\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "model = load_trained_model_from_checkpoint(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    training=False,\n",
        "    trainable=False,\n",
        "    seq_len=SEQ_LEN,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8UENTUWa9RB"
      },
      "source": [
        "#歐式距離函數\n",
        "import numpy as np\n",
        "def dist(x, y):\n",
        "    return np.sqrt(((x - y)**2).sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUv_cyA3In-S"
      },
      "source": [
        "#使用BERT訓練過的詞向量預測被mask的詞組\r\n",
        "vectors = model.predict([input_ids, input_seg])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArwzEE_hT3oX"
      },
      "source": [
        "#準備關聯性列表\r\n",
        "d_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uop_7NHsIXIj",
        "outputId": "7bcf975c-4cb9-4787-a384-ffd8adbfc4e0"
      },
      "source": [
        "#計算每一詞組中,二個詞的關聯性,將結果輸出到列表中\n",
        "import numpy as np\n",
        "threshold = 10\n",
        "for n in range(1, round(len(position_list)/3+1)):\n",
        "    v1 = np.zeros_like(vectors.shape[2]).astype('float32')\n",
        "    v2_1 = np.zeros_like(vectors.shape[2]).astype('float32')\n",
        "    v2_2 = np.zeros_like(vectors.shape[2]).astype('float32')\n",
        "    v3 = np.zeros_like(vectors.shape[2]).astype('float32')\n",
        "    len_of_token_1 = 0\n",
        "    len_of_token_2 = 0\n",
        "    for m in range(len(position_list[3*n-2])):\n",
        "      if position_list[3*n-3][m]==1:\n",
        "        v1 = v1 + vectors[3 * n-3][m]\n",
        "        v2_1 = v2_1 + vectors[3 * n-2][m]\n",
        "        len_of_token_1 += 1\n",
        "      elif position_list[3*n-1][m]==1:\n",
        "        v3 = v3 + vectors[3 * n-1][m]\n",
        "        v2_2 = v2_2 + vectors[3 * n-2][m]\n",
        "        len_of_token_2 += 1\n",
        "    v1 = v1/len_of_token_1\n",
        "    v2_1 = v2_1/len_of_token_1\n",
        "    v2_2 = v2_2/len_of_token_2\n",
        "    v3 = v3/len_of_token_2  \n",
        "    d1 = dist(v1, v2_1)\n",
        "    d2 = dist(v2_2 ,v3)\n",
        "    d = (d1 + d2) / 2\n",
        "    d_list.append(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['有', '价值']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4KeAWg1TZUZ"
      },
      "source": [
        "#讀取濾除字表\r\n",
        "stop_words_list = []\r\n",
        "with open('stop_words.txt', 'r') as f:\r\n",
        "  stop_words_list = [line.strip() for line in f]\r\n",
        "  t = f.readlines()\r\n",
        "  for i in t:\r\n",
        "    stop_words_list.append(cc.convert(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUo7L0EcTSpV"
      },
      "source": [
        "#過濾字表中的字後,根據關連性的大小,尋找對應的詞組,輸出段落中最可能是相連接的詞組\r\n",
        "for n in range(len(d_list)):\r\n",
        "  k = d_list.index(max(d_list))\r\n",
        "  if word_pair_list[k][0] not in stop_words_list and word_pair_list[k][1] not in stop_words_list:\r\n",
        "    with open(\"new_keyword.txt\", \"a\") as output:\r\n",
        "      output.write(\"%s\" % word_pair_list[k])\r\n",
        "    print(word_pair_list[k])\r\n",
        "    break\r\n",
        "  else:\r\n",
        "    del d_list[k]\r\n",
        "    del word_pair_list[k]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}